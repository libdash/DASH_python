---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---


# {index}`GSS 2020 data <single: GSS 2020 data>`

```{index} single: Python; GSS 2020 data
```

## Overview

In this section we perform data analysis on 2020 GSS data. We cover the following concepts:

* Univariate/Bivariate analysis
* Data cleaning 
* Data manipulation
* Anova
* etc


### Univariate/Bivariate analysis

Univariate and bivariate analyses are essential steps in exploratory data analysis (EDA). They help you understand the distribution and relationships of variables in your dataset. Here’s a step-by-step guide for each:

#### Univariate Analysis

Univariate analysis examines the distribution of a single variable. The goal is to understand the central tendency, dispersion, and shape of the variable’s distribution.

Steps:
1. Identify the Variable Type:
   * Categorical (e.g., gender, color): Non-numeric data that represent categories.
   * Numerical (e.g., age, income): Numeric data that can be discrete or continuous.
  
2. Descriptive Statistics:
   
   * Categorical Variables:
     * Frequency Distribution: Count the occurrences of each category.
     * Mode: The category with the highest frequency.
   * Numerical Variables:
     * Measures of Central Tendency: Mean, median, mode.
     * Measures of Dispersion: Range, variance, standard deviation, interquartile range (IQR).
     * Skewness & Kurtosis: To assess the shape of the distribution.
  
3. Visualization:
   * Categorical Variables:
     * Bar Chart
     * Pie Chart
   * Numerical Variables:
     * Histogram
     * Boxplot
     * Density Plot
  
4. Check for Outliers: (Numerical variables)
   * Use boxplots or z-scores to identify and assess outliers.

5. Assess Distribution: (Numerical variables)
   * Identify if the data is normally distributed or skewed using histograms and Q-Q plots.

#### Bivariate Analysis
Bivariate analysis examines the relationship between two variables. The aim is to identify patterns, correlations, or associations between them.

Steps:
1. Identify the Variable Types:

   * Both Categorical
   * Both Numerical
   * One Categorical and One Numerical
  
2. Descriptive Statistics:
   
   * Categorical vs. Categorical:
     * Cross-tabulation: Create a contingency table to observe the frequency of combinations of categories.
     * Chi-Square Test: Test the independence of the variables.
   * Numerical vs. Numerical:
     * Correlation Coefficient: Use Pearson (linear relationships) or Spearman (non-linear relationships) to measure the strength and direction of the relationship.
     * Covariance: Assess the direction of the relationship (positive or negative).
   * Categorical vs. Numerical:
        * Group Statistics: Calculate summary statistics (mean, median, etc.) for the numerical variable across different categories.
        * T-test or ANOVA: Test the difference in means between groups.
3. Visualization:

   * Categorical vs. Categorical:
     * Stacked Bar Chart
     * Mosaic Plot
   * Numerical vs. Numerical:
        * Scatter Plot
        * Line Chart (if time-series data)
        * Pair Plot (for multiple numerical variables)
   * Categorical vs. Numerical:
        * Boxplot (by category)
        * Violin Plot
        * Bar Plot with error bars
  
4. Identify Relationships:

   * Categorical vs. Categorical: Look for patterns or significant associations in the contingency table.
   * Numerical vs. Numerical: Look for trends, clusters, or correlations in the scatter plot.
   * Categorical vs. Numerical: Check for differences in distributions or central tendencies across categories.
5. Hypothesis Testing:

   * Conduct appropriate statistical tests to validate or reject hypotheses about the relationships between variables.


### Step 1:

If necessary, select your sample: 
For example we select those who's age are 25+

```{code-cell} ipython
import pandas as pd
df = pd.read_csv("/Users/amirrezamousavi/Desktop/lisa/Microdata Coding Project/gss2020_small_new.csv")
filtered_df = df[df['AGEGR10'] > 2]
```

We can run a frequency table for agegr10  to check the sample selection was done correctly.

```{code-cell} ipython
frequency_table = filtered_df['AGEGR10'].value_counts().sort_index()
print(frequency_table)
```

We can also check to see if we have missing values:

```{code-cell} ipython
print("number of missing values for each feature:",df.isna().sum())
```


### Step 2:

Removing missing values:

In pandas, the [dropna()]('https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html') method is used to remove missing values (i.e., NaN values) from a DataFrame or Series. 

```{code-cell} ipython
# Remove rows with any missing values
df_cleaned = filtered_df.dropna()
```

To obtain the frequency distribution of the variable vismin_c

```{code-cell} ipython
# Get frequency distribution
frequency_distribution = filtered_df['VISMIN_C'].value_counts()

print(frequency_distribution)               
```

```{code-cell} ipython
# Create new variable vismin2 as a copy of vismin_c
import warnings
warnings.filterwarnings('ignore')
filtered_df['vismin2'] = filtered_df['VISMIN_C']             
```

recode vismin2(1 thru 9=1)(10=0)(999=sysmis). 

```{code-cell} ipython
import numpy as np
# Recode values
filtered_df['vismin2'] = filtered_df['vismin2'].replace({999: np.nan})  # Set 999 to NaN (sysmis)
filtered_df['vismin2'] = filtered_df['vismin2'].apply(lambda x: 1 if 1 <= x <= 9 else (0 if x == 10 else np.nan))        
```

In the next step we give value label to each category of the new var vismin2. 
Value labels vismin2 0'not a visible minority' 1'visible minority'. 


```{code-cell} ipython
labels = {0: 'Not a Visible Minority', 1: 'Visible Minority'}
# Apply labels
filtered_df['vismin2_label'] = filtered_df['vismin2'].map(labels)     
```

In this section as an example we reverse code GEN_01 variable.
recode genhealth_rev(1=5)(2=4)(3=3)(4=2)(5=1)(9=sysmis). 
value labels genhealth_rev 1'poor' 2'fair' 3'good' 4'very good' 5'excellent'. 

```{code-cell} ipython
# Reverse code the variable
filtered_df['genhealth_rev'] = filtered_df['GEN_01'].replace({9: np.nan})  # Set 9 to NaN (sysmis)
filtered_df['genhealth_rev'] = filtered_df['genhealth_rev'].apply(lambda x: 6 - x if pd.notna(x) else np.nan)

# Define labels
labels = {1: 'Poor', 2: 'Fair', 3: 'Good', 4: 'Very Good', 5: 'Excellent'}

# Apply labels
filtered_df['genhealth_rev_label'] = filtered_df['genhealth_rev'].map(labels)

```

Since ratio variables are limited in the GSS data I am recoding SCF100_C (ordinal) into a ratio

```{code-cell} ipython
filtered_df['numfriends'] = filtered_df['SCF100_C'].replace({9: np.nan})  # Assuming 9 indicates missing values
```

recode numfriends(10=14.5)(11=24.5)(12=30)(999=sysmis). 

```{code-cell} ipython
filtered_df['numfriends'] = filtered_df['numfriends'].replace({
    10: 14.5,
    11: 24.5,
    12: 30,
    999: np.nan  # Treat 999 as missing
})
```

To analyze the 'numfriends' variable and its relationship with 'SCF100_C', you need to generate a frequency distribution and a cross-tabulation table.



```{code-cell} ipython
# Frequencies
frequency_distribution = filtered_df['numfriends'].value_counts(dropna=False)

print("Frequencies of numfriends:")
print(frequency_distribution)

# Cross-tabulation
crosstab = pd.crosstab(filtered_df['numfriends'], filtered_df['SCF100_C'], rownames=['numfriends'], colnames=['SCF100_C'])

print("\nCross-tabulation of numfriends by SCF100_C:")
print(crosstab)
```


For univariate analysis of a ratio variable such as numfriends, you can calculate various descriptive statistics including frequencies, mean, median, standard deviation, and mode:

```{code-cell} ipython
# Frequencies
frequency_distribution = filtered_df['numfriends'].value_counts(dropna=False)

# Descriptive statistics
mean = filtered_df['numfriends'].mean()
median = filtered_df['numfriends'].median()
stddev = filtered_df['numfriends'].std()
mode = filtered_df['numfriends'].mode().values  # mode() returns a Series, take the values

print("Frequencies of numfriends:")
print(frequency_distribution)

print("\nDescriptive Statistics for numfriends:")
print(f"Mean: {mean}")
print(f"Median: {median}")
print(f"Standard Deviation: {stddev}")
print(f"Mode: {mode}")

```

Bivariate analysis: 
t-test (comparing mean # of friends between visible minority and non-visible minority). 

 ```{code-cell} ipython
from scipy import stats

# Split data into two groups
group1 = filtered_df[filtered_df['vismin2_label'] == 'Not a Visible Minority']['numfriends']
group2 = filtered_df[filtered_df['vismin2_label'] == 'Visible Minority']['numfriends']

# Perform the t-test
t_stat, p_value = stats.ttest_ind(group1, group2, nan_policy='omit')

print("T-test Results:")
print(f"T-statistic: {t_stat}")
print(f"P-value: {p_value}")
```

 t-test groups vismin2(0 1)/variables numfriends:
 ```{code-cell} ipython
from scipy import stats
# Split data into two groups
group1 = filtered_df[filtered_df['vismin2'] == 0]['numfriends']
group2 = filtered_df[filtered_df['vismin2'] == 1]['numfriends']

# Perform the t-test
t_stat, p_value = stats.ttest_ind(group1, group2, nan_policy='omit')

print("T-test Results:")
print(f"T-statistic: {t_stat}")
print(f"P-value: {p_value}")

```

## ANOVA

ANOVA, or Analysis of Variance, is a statistical technique used to determine if there are any statistically significant differences between the means of three or more independent groups. It is commonly used when comparing more than two groups to see if at least one group mean is different from the others.

### Key Concepts:
* Null Hypothesis (H0): Assumes that all group means are equal.
* Alternative Hypothesis (H1): Assumes that at least one group mean is different.
* F-statistic: The ratio of the variance between the group means to the variance within the groups. A higher F-statistic suggests that the group means are not all the same.
* p-value: Helps determine the significance of the results. A p-value less than a chosen significance level (e.g., 0.05) leads to rejection of the null hypothesis.
### Types of ANOVA:
1. One-Way ANOVA: Compares the means of three or more independent groups based on one independent variable.
1. Two-Way ANOVA: Examines the influence of two different independent variables on the dependent variable and can also assess the interaction between these variables.
1. Repeated Measures ANOVA: Used when the same subjects are tested under different conditions (e.g., time points).
### Example of One-Way ANOVA:
Imagine a study where researchers want to test the effect of different diets on weight loss. The independent variable is the type of diet (e.g., Diet A, Diet B, Diet C), and the dependent variable is weight loss. ANOVA can determine if there is a statistically significant difference in weight loss between the diet groups.

### Steps to Perform ANOVA:
1. State the hypotheses: Formulate the null and alternative hypotheses.
2. Calculate the F-statistic: Use statistical software or formulas to compute the F-statistic.
3. Compare the F-statistic to the critical value: Determine whether to reject or fail to reject the null hypothesis.
4. Post hoc tests (if necessary): If ANOVA indicates a significant difference, post hoc tests (like Tukey's HSD) can determine which specific groups are different.


For our dataframe we want to compare mean # of friends between racial groups (original vismin_c). 

 ```{code-cell} ipython
from scipy import stats

# Drop rows with NaN values in the relevant columns
anova_data = filtered_df[['numfriends', 'VISMIN_C']].dropna()

# Perform one-way ANOVA
anova_results = stats.f_oneway(
    *[anova_data['numfriends'][anova_data['VISMIN_C'] == group] for group in anova_data['VISMIN_C'].unique()]
)

print("ANOVA F-statistic:", anova_results.statistic)
print("ANOVA p-value:", anova_results.pvalue)
```

### Interpreting the reuslts:

* F-statistic: This tells you whether there is a statistically significant difference in the means across the groups.
* p-value: If the p-value is below a certain threshold (commonly 0.05), you can reject the null hypothesis that the means are equal across the groups

## Crosstab

In this section we perform crosstab for nominal (visible minority) and ordinal variable (highest degree). 
1. create crosstab:

 ```{code-cell} ipython
import pandas as pd
crosstab = pd.crosstab(filtered_df['ED_05'], filtered_df['VISMIN_C'], margins=True, normalize='columns')
print(crosstab)
```
2. Chi-Square Test

To assess the association between the two variables, you can perform a chi-square test.

 ```{code-cell} ipython
from scipy.stats import chi2_contingency
chi2, p, dof, ex = chi2_contingency(crosstab.iloc[:-1, :-1])

print(f"Chi-Square Test Statistic: {chi2}")
print(f"P-value: {p}")
print(f"Degrees of Freedom: {dof}")
```

3. Interpret the Results:
   
* Chi-Square Statistic: Measures how the observed counts diverge from the expected counts.
* P-value: If this value is below 0.05, it suggests a statistically significant association between the two variables.
* Degrees of Freedom (DoF): This is calculated based on the number of categories in each variable.
